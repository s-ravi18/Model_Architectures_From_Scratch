{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wiqGn-IzxhbO"
      ],
      "authorship_tag": "ABX9TyMw9FUoNBwMYuSOBDUPV2IB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s-ravi18/Model_Architectures_From_Scratch/blob/main/LLMs/Hands_on_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding the RNN from scratch;"
      ],
      "metadata": {
        "id": "wiqGn-IzxhbO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdoA10XltdNT"
      },
      "outputs": [],
      "source": [
        "## Coding the RNN from scratch;\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 0.\n",
        "## initialise the hidden state (h0) - weights from a normal curve;\n",
        "## initialise the weight matrix for hidden state (Wh) - weights from a normal curve;\n",
        "## initialise the weight matrix for input vector (Wx) - weights from a normal curve;\n",
        "\n",
        "# Step 1.\n",
        "\n",
        "## Pick up the first token of the sequence;\n",
        "\n",
        "## calcuate the output after consuming first token\n",
        "## calculate the new hidden state\n",
        "\n",
        "## ht = F(Wh * ht-1 + Wx * Xt)\n",
        "\n",
        "# Step 2.\n",
        "\n",
        "## Pick up the next token of the sequence\n",
        "\n",
        "## calcuate the output after consuming first token\n",
        "## calculate the new hidden state\n",
        "\n",
        "# Step 3. Repeat Step 2 n times\n",
        "\n",
        "\n",
        "## The final hidden state after processing the last token of the input sequence, after multiple iterations would be the encoded vector which is passed to the\n",
        "## decoder\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Rau8EQprteC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FC3slSnNteFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Akc7rQxgteHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Coding LSTMs from scratch;\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    e = np.exp(x - np.max(x))\n",
        "    return e / np.sum(e)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "class LSTMCell:\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # weight matrices\n",
        "        self.Wf = np.random.randn(hidden_size, hidden_size + input_size) * 0.1\n",
        "        self.Wi = np.random.randn(hidden_size, hidden_size + input_size) * 0.1\n",
        "        self.Wo = np.random.randn(hidden_size, hidden_size + input_size) * 0.1\n",
        "        self.Wc = np.random.randn(hidden_size, hidden_size + input_size) * 0.1\n",
        "\n",
        "        # biases\n",
        "        self.bf = np.zeros((hidden_size, 1))\n",
        "        self.bi = np.zeros((hidden_size, 1))\n",
        "        self.bo = np.zeros((hidden_size, 1))\n",
        "        self.bc = np.zeros((hidden_size, 1))\n",
        "\n",
        "    def forward(self, x_t, h_prev, C_prev):\n",
        "        # concatenate h(t-1) and x(t)\n",
        "        concat = np.vstack((h_prev, x_t))\n",
        "\n",
        "        # gates\n",
        "        f_t = sigmoid(self.Wf @ concat + self.bf)\n",
        "        i_t = sigmoid(self.Wi @ concat + self.bi)\n",
        "        o_t = sigmoid(self.Wo @ concat + self.bo)\n",
        "        C_hat = np.tanh(self.Wc @ concat + self.bc)\n",
        "\n",
        "        # new cell and hidden state\n",
        "        C_t = f_t * C_prev + i_t * C_hat\n",
        "        h_t = o_t * np.tanh(C_t)\n",
        "\n",
        "        return h_t, C_t\n",
        "\n",
        "# ------------ Encoder --------------------------\n",
        "\n",
        "class Encoder:\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        self.lstm = LSTMCell(input_size, hidden_size)\n",
        "\n",
        "    def encode(self, X):\n",
        "        h = np.zeros((self.lstm.hidden_size, 1))\n",
        "        C = np.zeros((self.lstm.hidden_size, 1))\n",
        "\n",
        "        for x_t in X:   ### Sequential scanning of the input and updating cell and hidden state.\n",
        "            x_t = x_t.reshape(-1, 1)\n",
        "            h, C = self.lstm.forward(x_t, h, C)\n",
        "\n",
        "        return h, C  # context vector\n",
        "\n",
        "# ------------ Decoder --------------------------\n",
        "\n",
        "class Decoder:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.lstm = LSTMCell(input_size, hidden_size)\n",
        "        self.Wy = np.random.randn(output_size, hidden_size) * 0.1\n",
        "        self.by = np.zeros((output_size, 1))\n",
        "\n",
        "    def decode(self, context_h, context_C, max_len, start_vector, vocab):\n",
        "        h, C = context_h, context_C\n",
        "        x_t = start_vector.reshape(-1, 1)\n",
        "\n",
        "        outputs = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            h, C = self.lstm.forward(x_t, h, C)\n",
        "            y_pred = softmax(self.Wy @ h + self.by)\n",
        "            token = np.argmax(y_pred)\n",
        "            outputs.append(token)\n",
        "\n",
        "            # next input = embedding of predicted token\n",
        "            x_t = vocab[token].reshape(-1, 1)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "BFnharuW7t2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------ Tiny Translation Example ---------\n",
        "\n",
        "# Fake embeddings for demonstration\n",
        "embed = {\n",
        "    \"i\": np.array([1,0,0], dtype=float),\n",
        "    \"love\": np.array([0,1,0], dtype=float),\n",
        "    \"apples\": np.array([0,0,1], dtype=float),\n",
        "\n",
        "    # French tokens\n",
        "    \"<SOS>\": np.array([1,1,1], dtype=float),\n",
        "    \"j'\": np.array([1,0,1], dtype=float),\n",
        "    \"aime\": np.array([0,1,1], dtype=float),\n",
        "    \"les\": np.array([1,1,0], dtype=float),\n",
        "    \"pommes\": np.array([0,1,0], dtype=float)\n",
        "}\n",
        "\n",
        "# vocab index for decoding\n",
        "vocab_list = [\"j'\", \"aime\", \"les\", \"pommes\"]\n",
        "vocab_vectors = [embed[v] for v in vocab_list]\n",
        "\n",
        "# Encoder input (English sentence)\n",
        "X = [\n",
        "    embed[\"i\"],\n",
        "    embed[\"love\"],\n",
        "    embed[\"apples\"]\n",
        "]"
      ],
      "metadata": {
        "id": "rUm750qd87mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(input_size=3, hidden_size=4)\n",
        "decoder = Decoder(input_size=3, hidden_size=4, output_size=len(vocab_list))\n",
        "\n",
        "context_h, context_C = encoder.encode(X)\n",
        "\n",
        "# Decode to max length 4\n",
        "output_ids = decoder.decode(\n",
        "    context_h=context_h,\n",
        "    context_C=context_C,\n",
        "    max_len=4,\n",
        "    start_vector=embed[\"<SOS>\"],\n",
        "    vocab=vocab_vectors\n",
        ")\n",
        "\n",
        "translated_tokens = [vocab_list[i] for i in output_ids]\n",
        "print(\"Predicted translation:\", translated_tokens)\n"
      ],
      "metadata": {
        "id": "pu2gxtac7t4w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c4f1419-21c1-4d28-fc86-cf781b8c73c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted translation: [\"j'\", 'pommes', \"j'\", 'pommes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5n87aX0lr9ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fsy-YXIGr9g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ShB2ZLhpr9jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding the GPT-2 from scratch;"
      ],
      "metadata": {
        "id": "f5Vui_oGxnJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Coding the GPT-2 from scratch;\n",
        "\n",
        "## ENCODER\n",
        "\n",
        "## Tokenisation\n",
        "## Text Embedding\n",
        "## Position Embedding - For parallelism (could be learnt parameters)\n",
        "\n",
        "## Multi Dimensions - 32/64/...\n",
        "## Self Attention Mechanism - each k, q, v is of dimension (max_len, d_head) ## unsqueeze\n",
        "## Concatenating the outputs of all the heads\n",
        "\n",
        "## MLP - GELU as the activation function\n",
        "\n",
        "## The K, Q are reused in the decoder;\n",
        "\n",
        "## DECODER;\n",
        "\n",
        "## Self-Attention mechanism - Masked Inferencing - incorporates the lower triangle matrix while attention calculation\n",
        "## MLP\n",
        "## Final output - probabilityfor each word in the vocab\n",
        "## Top K, Greedy, etc... ways of token sampling/selection\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qQv5We-Vr9l_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UOw-2BWAr9oW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Ti3Ribgtvp3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BUILDING THE ENCODER BLOCK;\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_k):\n",
        "        super().__init__()\n",
        "        self.scale = d_k ** 0.5\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        # Q, K, V: (batch, heads, seq_len, d_k)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        output = torch.matmul(weights, V)\n",
        "        return output, weights\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0)/d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        return x + self.pe[:, :seq_len].to(x.device)\n",
        "\n",
        "\n",
        "# self.pos_emb = nn.Embedding(max_len, d_model)\n",
        "# pos_ids = torch.arange(0, seq_len).unsqueeze(0).to(x.device)\n",
        "# x = x + self.pos_emb(pos_ids)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        # Linear layers to project input → Q, K, V\n",
        "        self.W_Q = nn.Linear(d_model, d_model)\n",
        "        self.W_K = nn.Linear(d_model, d_model)\n",
        "        self.W_V = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Output linear projection\n",
        "        self.W_O = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # Project to Q, K, V\n",
        "        Q = self.W_Q(x)  # (batch, seq, d_model)\n",
        "        K = self.W_K(x)\n",
        "        V = self.W_V(x)\n",
        "\n",
        "        # Reshape for multi-heads\n",
        "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Weighted sum\n",
        "        attention = torch.matmul(weights, V)  # (batch, heads, seq, d_k)\n",
        "\n",
        "        # Concatenate heads\n",
        "        attention = attention.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "        # Final projection\n",
        "        out = self.W_O(attention)\n",
        "\n",
        "        return out, weights\n",
        "\n",
        "\n",
        "\n",
        "# x = torch.rand(2, 5, 512)\n",
        "# mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
        "# out, weights = mha(x)\n",
        "# print(out.shape)      # expected (2, 5, 512)\n",
        "# print(weights.shape)  # expected (2, 8, 5, 5)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, d_model),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model=512, num_heads=8, hidden_dim=2048):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.ffn = FeedForward(d_model, hidden_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Attention block\n",
        "        attn_out, _ = self.mha(self.ln1(x), mask)\n",
        "        x = x + attn_out  # residual\n",
        "\n",
        "        # FFN block\n",
        "        ffn_out = self.ffn(self.ln2(x))\n",
        "        x = x + ffn_out  # residual\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "z482yVNjtvsJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r6Ut5fvQB8Sn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DECODER BLOCK;\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model=512, num_heads=8, hidden_dim=2048):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.ffn = FeedForward(d_model, hidden_dim)\n",
        "\n",
        "    def forward(self, x, causal_mask):\n",
        "        # Self-attention with causal mask\n",
        "        attn_out, _ = self.self_attn(self.ln1(x), mask=causal_mask)\n",
        "        x = x + attn_out\n",
        "\n",
        "        # Feed-forward\n",
        "        ffn_out = self.ffn(self.ln2(x))\n",
        "        x = x + ffn_out\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def causal_mask(seq_len):\n",
        "    mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)\n",
        "    return mask  # shape: (1,1,seq,seq)\n"
      ],
      "metadata": {
        "id": "EojijEB2uo2f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MiniEncoder(nn.Module):\n",
        "    def __init__(self, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderBlock() for _ in range(num_layers)])\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "0Xy1N9FUuo4v"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vySoHjMtxrlB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b3sC1bdtxrnn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mIEv7HNNxrqH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Application 1. Training an Encoder only architecure;\n",
        "\n",
        "- Used for Analysing context/patterns (Understanding)\n",
        "\n",
        "- Example: Sentence Classification/Similar Embedding creation, etc..."
      ],
      "metadata": {
        "id": "U1bvUryhxsV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input → Token Embedding + Positional Encoding\n",
        "#       → N EncoderBlocks\n",
        "#       → CLS output\n",
        "#       → Linear Layer → Softmax"
      ],
      "metadata": {
        "id": "JViE7TQ_xrsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MiniBERT(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, n_heads=4, n_layers=2, hidden_dim=256, max_len=128, num_labels=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderBlock(d_model, n_heads, hidden_dim)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "\n",
        "# class EncoderBlock(nn.Module):\n",
        "#     def __init__(self, d_model=512, num_heads=8, hidden_dim=2048):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.ln1 = nn.LayerNorm(d_model)\n",
        "#         self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "#         self.ln2 = nn.LayerNorm(d_model)\n",
        "#         self.ffn = FeedForward(d_model, hidden_dim)\n",
        "\n",
        "#     def forward(self, x, mask=None):\n",
        "#         # Attention block\n",
        "#         attn_out, _ = self.mha(self.ln1(x), mask)\n",
        "#         x = x + attn_out  # residual\n",
        "\n",
        "#         # FFN block\n",
        "#         ffn_out = self.ffn(self.ln2(x))\n",
        "#         x = x + ffn_out  # residual\n",
        "\n",
        "#         return x\n",
        "\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.classifier = nn.Linear(d_model, num_labels)\n",
        "\n",
        "    def forward(self, input_ids):  ## (1, 5)\n",
        "        B, T = input_ids.shape\n",
        "        pos = torch.arange(0, T).unsqueeze(0).to(input_ids.device)  ## (1, 5)\n",
        "\n",
        "        x = self.token_emb(input_ids) + self.pos_emb(pos)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        cls_token = x[:, 0, :]  # use first token representation\n",
        "\n",
        "        logits = self.classifier(cls_token)\n",
        "        return logits\n",
        "\n",
        "## Sample data;\n",
        "sentences = [\n",
        "    \"I love this movie\",\n",
        "    \"This film is terrible\",\n",
        "    \"Amazing acting!\",\n",
        "    \"Worst story ever\"\n",
        "]\n",
        "\n",
        "labels = [1, 0, 1, 0]   # 1 = positive, 0 = negative\n",
        "\n",
        "\n",
        "\n",
        "word2id = {\"<pad>\":0, \"<cls>\":1}\n",
        "\n",
        "## Tokenising\n",
        "for s in sentences:\n",
        "    for w in s.lower().split(\" \"):\n",
        "        if w not in word2id:\n",
        "            word2id[w] = len(word2id)\n",
        "\n",
        "def encode(sentence):\n",
        "    tokens = [word2id[w] for w in sentence.lower().split()]\n",
        "    return torch.tensor([1] + tokens)   # prepend CLS\n"
      ],
      "metadata": {
        "id": "W-CtJIdtx-Q7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MiniBERT(vocab_size=len(word2id))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)  ## for learning the parameters...\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1000):\n",
        "    total_loss = 0\n",
        "\n",
        "    for s, y in zip(sentences, labels):\n",
        "        x = encode(s).unsqueeze(0)  ### (5, ) --> (1, 5)\n",
        "        y = torch.tensor([y])\n",
        "\n",
        "        logits = model(x)\n",
        "        loss = loss_fn(logits, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(\"Epoch:\", epoch, \"Loss:\", total_loss)\n"
      ],
      "metadata": {
        "id": "9RijNLEYx-TV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iMRdPCsLx-Vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VdnnHrn3uo7Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}